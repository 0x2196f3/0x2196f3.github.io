<!doctype html><html lang=zh-CN data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><meta name=theme-name content="Anubis2"><title>docker部署llama-swap - 0x2196f3&rsquo;s blog</title><meta name=description content='gpt-oss刚发布，ollama的支持有问题，性能和llama-cpp相比有几倍的差距，但是llama-cpp一个进程只能加载一个gguf模型，api也比较简陋，无法长期使用
不过ollama性能问题估计过几天就修复了


    
        #
    
    Ollama Llama-cpp 性能对比



  
      
          gpt-oss:20b
          tokens/s
      
  
  
      
          llama-cpp (vulkan)
          96.52
      
      
          llama-cpp (rocm on Windows)
          94.11
      
      
          ollama (rocm)
          35.72
      
  



    
        #
    
    llama-swap


llama-server和llama-swap打包进同一个docker，通过启动多个llama-server进程切换gguf模型，提供一个简单的webui，可以手动加载/卸载模型，也可以在网页上对话
由于llama-cpp没有提供Linux的ROCM二进制文件，只能使用Vulkan，单卡性能和ROCM基本没有区别
Vulkan可以在多张不同品牌的GPU上运行同一个模型，实测一张AMD独显+一张Intel独显能运行，性能可以接受 (16GB+16GB，qwq:32b Q4_K_M 15 token/s)


    
        #
    
    Installation



AMD显卡安装驱动+ROCM (Ubuntu24)
文档
cd /tmp

# insttll driver
wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb
sudo apt install ./amdgpu-install_6.4.60402-1_all.deb
sudo apt update
sudo apt install "linux-headers-$(uname -r)" "linux-modules-extra-$(uname -r)"
sudo apt install amdgpu-dkms

#reboot
sudo reboot

# install rocm
wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb
sudo apt install ./amdgpu-install_6.4.60402-1_all.deb
sudo apt update
sudo apt install python3-setuptools python3-wheel
sudo usermod -a -G render,video $LOGNAME
sudo apt install rocm

#reboot
sudo reboot

AMD显卡部署llama-swap:vulkan'><link rel=icon type=image/x-icon href=https://0x2196f3.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://0x2196f3.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.ac9fbad24609bd533b00c98f0dfbd971989a71aca4b450e1e77ab6c33ee463c6.css integrity="sha256-rJ+60kYJvVM7AMmPDfvZcZiacayktFDh53q2wz7kY8Y="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>0x2196f3's blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://0x2196f3.github.io/ title>Home</a>
<a href=https://0x2196f3.github.io/tags/ title>Tags</a>
<a href=https://0x2196f3.github.io/posts/ title>Archive</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">docker部署llama-swap</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2025-08-08>2025-08-08</time></div><a class="post-hidden-url u-url" href=/posts/llama-swap-on-linux/>/posts/llama-swap-on-linux/</a>
<a href=https://0x2196f3.github.io/ class="p-name p-author post-hidden-author h-card" rel=me></a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/ai>#AI</a></li><li><a href=/tags/linux>#Linux</a></li><li><a href=/tags/server>#Server</a></li><li><a href=/tags/llm>#LLM</a></li><li><a href=/tags/docker>#docker</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#ollama-llama-cpp-性能对比>Ollama Llama-cpp 性能对比</a></li><li><a href=#llama-swap><a href=https://github.com/mostlygeek/llama-swap>llama-swap</a></a></li><li><a href=#installation>Installation</a></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>gpt-oss刚发布，ollama的支持有问题，性能和llama-cpp相比有几倍的差距，但是llama-cpp一个进程只能加载一个gguf模型，api也比较简陋，无法长期使用</p><p><del>不过ollama性能问题估计过几天就修复了</del></p><h2 id=ollama-llama-cpp-性能对比><div><a href=#ollama-llama-cpp-%e6%80%a7%e8%83%bd%e5%af%b9%e6%af%94>#
</a>Ollama Llama-cpp 性能对比</div></h2><table><thead><tr><th>gpt-oss:20b</th><th>tokens/s</th></tr></thead><tbody><tr><td>llama-cpp (vulkan)</td><td>96.52</td></tr><tr><td>llama-cpp (rocm on Windows)</td><td>94.11</td></tr><tr><td>ollama (rocm)</td><td>35.72</td></tr></tbody></table><h2 id=llama-swap><div><a href=#llama-swap>#
</a><a href=https://github.com/mostlygeek/llama-swap>llama-swap</a></div></h2><p>llama-server和llama-swap打包进同一个docker，通过启动多个llama-server进程切换gguf模型，提供一个简单的webui，可以手动加载/卸载模型，也可以在网页上对话</p><p>由于llama-cpp没有提供Linux的ROCM二进制文件，只能使用Vulkan，单卡性能和ROCM基本没有区别</p><p>Vulkan可以在多张不同品牌的GPU上运行同一个模型，实测一张AMD独显+一张Intel独显能运行，性能可以接受 (16GB+16GB，qwq:32b Q4_K_M 15 token/s)</p><h2 id=installation><div><a href=#installation>#
</a>Installation</div></h2><blockquote><p>AMD显卡安装驱动+ROCM (Ubuntu24)</p></blockquote><p><a href=https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html>文档</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd /tmp
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># insttll driver</span>
</span></span><span style=display:flex><span>wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb
</span></span><span style=display:flex><span>sudo apt install ./amdgpu-install_6.4.60402-1_all.deb
</span></span><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install <span style=color:#e6db74>&#34;linux-headers-</span><span style=color:#66d9ef>$(</span>uname -r<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span> <span style=color:#e6db74>&#34;linux-modules-extra-</span><span style=color:#66d9ef>$(</span>uname -r<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>sudo apt install amdgpu-dkms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#reboot</span>
</span></span><span style=display:flex><span>sudo reboot
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># install rocm</span>
</span></span><span style=display:flex><span>wget https://repo.radeon.com/amdgpu-install/6.4.2/ubuntu/noble/amdgpu-install_6.4.60402-1_all.deb
</span></span><span style=display:flex><span>sudo apt install ./amdgpu-install_6.4.60402-1_all.deb
</span></span><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install python3-setuptools python3-wheel
</span></span><span style=display:flex><span>sudo usermod -a -G render,video $LOGNAME
</span></span><span style=display:flex><span>sudo apt install rocm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#reboot</span>
</span></span><span style=display:flex><span>sudo reboot
</span></span></code></pre></div><blockquote><p>AMD显卡部署llama-swap:vulkan</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 所有gguf模型和config.yaml 都放在容器 /data</span>
</span></span><span style=display:flex><span>sudo docker run -it -d --device /dev/kfd --device /dev/dri -p 8080:8080 -v /path/to/gguf/location:/data --name llama-swap ghcr.io/mostlygeek/llama-swap:vulkan --config /data/config.yaml
</span></span></code></pre></div><blockquote><p>最简config.yaml配置</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>models</span>:
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;gpt-oss-20b&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cmd</span>: <span style=color:#ae81ff>/app/llama-server --port ${PORT} -m /data/gpt-oss-20b.gguf -ngl 999</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;qwen3-30b&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>cmd</span>: <span style=color:#ae81ff>/app/llama-server  --port ${PORT} -m /data/qwen3-30b-q4_K_M.gguf -ngl 999</span>
</span></span></code></pre></div><blockquote><p>加载Gemma3的Vision功能</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    --<span style=color:#ae81ff>mmproj mmproj-gemma3-f16.gguf</span>
</span></span></code></pre></div><blockquote><p>覆盖模型的Context Length</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>	-<span style=color:#ae81ff>c 131072</span>
</span></span></code></pre></div><ul><li>调整<code>-ngl 999</code>将显存装不下的层卸载到CPU，提高性能</li><li><a href=https://github.com/mostlygeek/llama-swap/wiki/Configuration>文档</a></li><li>文档没写的看<a href=https://github.com/mostlygeek/llama-swap/blob/main/docker/llama-swap.Containerfile>Dockerfile</a></li></ul></div></article><script>function detectCurrentScheme2(){const e="auto";return localStorage!==null&&localStorage.getItem("user-color-scheme")?localStorage.getItem("user-color-scheme"):e==="dark"||e==="light"?e:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}let giscusTheme=detectCurrentScheme2(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"0x2196f3/0x2196f3.github.io","data-repo-id":"R_kgDOPJtijw","data-category":"Announcements","data-category-id":"DIC_kwDOPJtij84Cs6CT","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",lazyload:"false",async:!0},main=document.querySelector("main"),giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([e,t])=>giscusScript.setAttribute(e,t)),main.appendChild(giscusScript)</script></main><footer class="common-footer noselect"><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© 2026</div><div style=display:flex;align-items:center;gap:4px></div></div><p class="h-card vcard"><a href=https://0x2196f3.github.io/ class="p-name u-url url fn" rel=me></a></p></footer></div></body></html>